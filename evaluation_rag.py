import asyncio
import os
import pandas as pd
from dotenv import load_dotenv
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset

from server.diagnosis.query import chat_diagnosis_report
from server.models.db_models import ChatMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

load_dotenv()


TEST_DOC_ID = "065b6e48-bc6f-41b5-bb86-1a90697872b7" 
TEST_USERNAME = "tester" 

test_questions = [
    "What is the total cholesterol level for Mrs. Priyani Almeda?",
    "Are there any abnormal results in the lipid profile? Which ones are high?",
    "When was this sample collected and who referred the patient?",
    "What is the HDL to LDL ratio listed in the report?",
    "Based on the target levels provided, is the LDL cholesterol considered optimal?"
]


ground_truths = [
    "The total cholesterol level is 165 mg/dL.",
    "Yes, there are abnormal results. The Triglycerides are 244 mg/dL (Reference: 10-200) and VLDL Cholesterol is 48.8 mg/dL (Reference: 10-41). Both are above the reference range.",
    "The sample was collected on 05 Dec, 2025. The patient was referred by Kalubowila Hospital.",
    "The HDL/LDL ratio is 0.6.",
    "Yes, the LDL cholesterol is 71.2 mg/dL. According to the target levels, LDL < 100 mg/dL is considered Optimal."
]

async def generate_responses():
    """Runs the questions through your actual RAG pipeline"""
    data_samples = {
        'question': [],
        'answer': [],
        'contexts': [],
        'ground_truth': ground_truths 
    }

    print(f"ðŸš€ Starting evaluation on {len(test_questions)} questions...")

    for i, question in enumerate(test_questions):
        print(f"Processing ({i+1}/{len(test_questions)}): {question}")
        
        # Mock the message format your API expects
        mock_messages = [ChatMessage(role="user", content=question)]
        
        try:
            # Call your actual backend function
            response = await chat_diagnosis_report(TEST_USERNAME, TEST_DOC_ID, mock_messages)
            
            data_samples['question'].append(question)
            data_samples['answer'].append(response["diagnosis"])
            data_samples['contexts'].append(response["contexts"])
            
        except Exception as e:
            print(f"âŒ Error on question '{question}': {e}")
            # If backend fails (502 error), fill with placeholder to prevent crash
            data_samples['question'].append(question)
            data_samples['answer'].append("I could not retrieve the information due to a server error.")
            data_samples['contexts'].append(["No context retrieved"])

    return data_samples

def run_evaluation():

    raw_data = asyncio.run(generate_responses())

    dataset = Dataset.from_dict(raw_data)

    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]

    judge_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    judge_embeddings = OpenAIEmbeddings()

    print("\nðŸ§  Running Ragas Evaluation (this may take a minute)...")

    results = evaluate(
        dataset, 
        metrics=metrics, 
        llm=judge_llm, 
        embeddings=judge_embeddings
    )

    print("\nðŸ“Š ============ EVALUATION RESULTS ============")
    print(results)
    
    df = results.to_pandas()
    
    # Check if columns exist before printing to avoid KeyError
    cols_to_print = ['question', 'faithfulness', 'answer_relevancy', 'context_precision','context_recall']
    existing_cols = [c for c in cols_to_print if c in df.columns]
    
    print("\nðŸ“ Detailed Breakdown:")
    print(df[existing_cols])
    
    df.to_csv("rag_evaluation_results.csv", index=False)
    print("\nâœ… Results saved to 'rag_evaluation_results.csv'")

if __name__ == "__main__":
    run_evaluation()